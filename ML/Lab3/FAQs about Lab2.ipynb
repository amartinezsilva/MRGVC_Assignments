{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOuDn+TWWkkr4/Cjpa/9/X5"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["## LAB SESSION 2 REINFORCEMENT LEARNING\n","Prepared by Rubén Martínez Cantín rmcantin@unizar.es"],"metadata":{"id":"CElbqfkKZX1z"}},{"cell_type":"markdown","metadata":{"id":"hgeC9EBYcoC0"},"source":["This comment is to help solve Questions 6 and later of Lab 2. They are based on the qlearningAgent.py file.\n","\n","First, if anyone is having trouble with Python, don't hesitate to contact me as soon as possible. This lab is about learning RL, not Python. Don't waste time trying to solve a Python bug or learning how to manage a dictionary.\n","\n","----\n","\n","In the `getQValue` method, they asked you to return the value of Q for a state-action pair and 0 otherwise. That is directly solved if you use a `util.Counter()` object.\n","\n","Because not all the state have all actions, using a 2D table or matrix for the Q-values is complicated. Instead, I recommend to use directly pairs of state-actions (Python tuples) as a single index. For example, you can make the `__init__()` and `getQValue()` like this:"]},{"cell_type":"code","metadata":{"id":"A5t6NodYcl9S"},"source":["    def __init__(self, **args):\n","        \"You can initialize Q-values here...\"\n","        ReinforcementAgent.__init__(self, **args)\n","        \"*** YOUR CODE HERE ***\"\n","        self.Qvalues = util.Counter()\n","        # You can add more variables here if needed.\n","\n","    def getQValue(self, state, action):\n","        \"\"\"\n","          Returns Q(state,action)\n","          Should return 0.0 if we have never seen a state\n","          or the Q node value otherwise\n","        \"\"\"\n","        \"*** YOUR CODE HERE ***\"\n","        # In this expression (state, action) is the tupla which is used as\n","        # index accessed with []\n","        return self.Qvalues[(state, action)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HCHqxdLne6as"},"source":["Of course, that is just a recommendation. If you have used other data structure, feel free to do it.\n","\n","*Important:* Grading will be based on correctness and the explanations in the report, not efficiency or code quality.\n","\n","----\n","\n","Note that two of the methods that you have to fill in Question 6, namely `computeValueFromQValues` and `computeActionFromQValues` are actually proxies for the optimal (greedy) policy (`getPolicy`) and the optimal values(`getValue`) once the Qvalues have converged.\n","\n","That structure will help us later to reuse the code for the robot Crawler and Pacman, and solving those with the same code.\n","\n","----\n","\n","Also, note the difference between `getPolicy` which gives the optimal (target) policy and `getAction` (with is filled in Question 7) which gives the $\\epsilon$-greedy (behaviour) policy.\n","\n","We want to learn `getPolicy`...\n","\n","...while doing `getAction` (or moving manually in Question 6)."]}]}